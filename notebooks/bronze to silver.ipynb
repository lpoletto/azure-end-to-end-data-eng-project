{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "690f6c67-1b7a-4b0e-89d8-e1321e737dfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, monotonically_increasing_id, expr\n",
    "from pyspark.sql.types import TimestampType, StructType, StructField, IntegerType, FloatType, DateType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edd27c53-a1e2-4f95-be9c-98bf6f6d1155",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Transform Product table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1939cd73-7fad-4bc8-8c78-4ab6a4cd88c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "product_path = '/mnt/bronze/SalesLT/Product/Product.parquet'\n",
    "categories_path = '/mnt/bronze/SalesLT/ProductCategory/ProductCategory.parquet'\n",
    "output_path = '/mnt/silver/SalesLT/STG_Products/'\n",
    "\n",
    "products_df = spark.read.parquet(product_path)\n",
    "categories_df = spark.read.parquet(categories_path)\n",
    "\n",
    "# Rename column 'Name' in categories_df\n",
    "categories_df = categories_df.withColumnRenamed(\"Name\", \"category_name\")\n",
    "\n",
    "stg_products_df = products_df \\\n",
    "    .join(categories_df, products_df[\"ProductCategoryID\"] == categories_df[\"ProductCategoryID\"], \"left\") \\\n",
    "    .withColumn(\"ProductKey\", monotonically_increasing_id() + 1) \\\n",
    "    .withColumn(\"ProductID\", col(\"ProductID\")) \\\n",
    "    .withColumn(\"ProductName\", col(\"Name\")) \\\n",
    "    .withColumn(\"Color\", col(\"Color\")) \\\n",
    "    .withColumn(\"ColorCategory\", expr(\"\"\"\n",
    "        CASE \n",
    "            WHEN Color IN ('Red', 'Orange', 'Yellow') THEN 'Warm'\n",
    "            WHEN Color IN ('Blue', 'Green', 'Purple') THEN 'Cool'\n",
    "            ELSE 'Neutral'\n",
    "        END\n",
    "    \"\"\")) \\\n",
    "    .withColumn(\"StandardCost\", col(\"StandardCost\")) \\\n",
    "    .withColumn(\"ListPrice\", col(\"ListPrice\")) \\\n",
    "    .withColumn(\"ProductMaxGrossProfit\", col(\"ListPrice\") - col(\"StandardCost\")) \\\n",
    "    .withColumn(\"CategoryName\", col(\"category_name\"))\n",
    "\n",
    "stg_products_df = stg_products_df.dropDuplicates()\n",
    "\n",
    "# Drop rows with ListPrice is 0\n",
    "stg_products_df = stg_products_df.select(col(\"ProductKey\"), col(\"ProductID\"), col(\"ProductName\"), col(\"Color\"), col(\"ColorCategory\"), col(\"StandardCost\"), col(\"ListPrice\"), col(\"ProductMaxGrossProfit\"), col(\"CategoryName\")).filter(col(\"ListPrice\") > 0)\n",
    "\n",
    "stg_products_df.write.format(\"delta\").mode(\"overwrite\").save(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7266c61d-543a-4d26-867e-962a6c21eeca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Transform Customer table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92d26563-ec42-4b72-891e-7aa3e3c6159e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, row_number\n",
    "\n",
    "path = '/mnt/bronze/SalesLT/Customer/Customer.parquet'\n",
    "output_path = '/mnt/silver/SalesLT/STG_Customers/'\n",
    "\n",
    "df = spark.read.parquet(path)\n",
    "column = df.columns\n",
    "\n",
    "# Get rows with EmailAddres is not null and FirsName is not null and LastName is not null\n",
    "filtered_df = df.select(    \n",
    "  col(\"CustomerID\"),\n",
    "  col(\"FirstName\"),\n",
    "  col(\"LastName\"),\n",
    "  col(\"EmailAddress\"),\n",
    "  col(\"Phone\"),\n",
    "  col(\"CompanyName\"),\n",
    "  col(\"ModifiedDate\")\n",
    ").filter(col(\"EmailAddress\").isNotNull())\n",
    "\n",
    "# Remove duplicate rows\n",
    "filtered_df = filtered_df.dropDuplicates()\n",
    "\n",
    "# Crear una ventana para generar una clave incremental\n",
    "window_spec = Window.orderBy(col(\"CustomerID\"))\n",
    "\n",
    "# Agregar la columna CustomerKey con valores autogenerados\n",
    "filtered_df_with_key = filtered_df.withColumn(\n",
    "    \"CustomerKey\",\n",
    "    row_number().over(window_spec)\n",
    ")\n",
    "\n",
    "filtered_df = filtered_df_with_key.select(\n",
    "  col(\"CustomerKey\"),    \n",
    "  col(\"CustomerID\"),\n",
    "  col(\"FirstName\"),\n",
    "  col(\"LastName\"),\n",
    "  col(\"EmailAddress\"),\n",
    "  col(\"Phone\"),\n",
    "  col(\"CompanyName\"),\n",
    ")\n",
    "\n",
    "filtered_df.write.format(\"delta\").mode(\"overwrite\").save(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "495b8a92-2667-4436-a49f-e0f62ff818a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Transform  SalesOrderDetail & SalesOrderHeader tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbacdd2d-43ce-4a34-b420-586429d85dff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, DateType, FloatType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "path_sales_order_detail = '/mnt/bronze/SalesLT/SalesOrderDetail/SalesOrderDetail.parquet'\n",
    "path_sales_order_header = '/mnt/bronze/SalesLT/SalesOrderHeader/SalesOrderHeader.parquet'\n",
    "output_path_sales_order_detail = '/mnt/silver/SalesLT/STG_SalesOrderDetail/'\n",
    "output_path_sales_order_header = '/mnt/silver/SalesLT/STG_SalesOrderHeader/'\n",
    "\n",
    "sales_order_detail_df = spark.read.parquet(path_sales_order_detail)\n",
    "\n",
    "clean_sales_order_detail_df = sales_order_detail_df.dropDuplicates()\n",
    "clean_sales_order_detail_df.write.format(\"delta\").mode(\"overwrite\").save(output_path_sales_order_detail)\n",
    "\n",
    "sales_order_header_df = spark.read.parquet(path_sales_order_header)\n",
    "clean_sales_order_header_df = sales_order_header_df.dropDuplicates()\n",
    "clean_sales_order_header_df.write.format(\"delta\").mode(\"overwrite\").save(output_path_sales_order_header)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "bronze to silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}